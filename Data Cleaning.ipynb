{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Become familiar with: \n",
    "* Handling various kind of data importing scenarios that is importing various kind of datasets (.csv, .txt), different kind of delimiters (comma, tab, pipe), and different methods (read_csv, read_table)\n",
    "* Getting basic information, such as dimensions, column names, and statistics summary\n",
    "* Getting basic data cleaning done that is removing NAs and blank spaces, imputing values to missing data points, changing a variable type, and so on\n",
    "* Creating dummy variables in various scenarios to aid modelling\n",
    "* Generating simple plots like scatter plots, bar charts, histograms, box plots, and so on\n",
    "\n",
    "# Outline\n",
    "\n",
    "## Reading the data – variations and examples\n",
    "\n",
    "### Data frames\n",
    "A data frame is one of the most common data structures available in Python. Data\n",
    "frames are very similar to the tables in a spreadsheet or a SQL table. In Python\n",
    "vocabulary, it can also be thought of as a dictionary of series objects (in terms of\n",
    "structure). A data frame, like a spreadsheet, has index labels (analogous to rows)\n",
    "and column labels (analogous to columns). It is the most commonly used pandas\n",
    "object and is a 2D structure with columns of different or same types. Most of the\n",
    "standard operations, such as aggregation, filtering, pivoting, and so on which can\n",
    "be applied on a spreadsheet or the SQL table can be applied to data frames using\n",
    "methods in pandas.\n",
    "\n",
    "### Delimiters \n",
    "A delimiter is a special character that separates various columns of a dataset from\n",
    "one another. The most common (one can go to the extent of saying that it is a default\n",
    "delimiter) delimiter is a comma (,). A .csv file is called so because it has comma\n",
    "separated values.\n",
    "\n",
    "## Various methods of importing data in Python\n",
    "\n",
    "### Case 1 – reading a dataset using the read_csv method\n",
    "\n",
    "#### The read_csv method\n",
    "The name of the method doesn't unveil its full might. It is a kind of misnomer in\n",
    "the sense that it makes us think that it can be used to read only CSV files, which is\n",
    "not the case. Various kinds of files, including .txt files having delimiters of various\n",
    "kinds can be read using this method.\n",
    "```\n",
    "pd.read_csv(filepath, sep=', ', dtype=None, header=None,\n",
    "skiprows=None, index_col=None, skip_blank_lines=TRUE, na_filter=TRUE)\n",
    "```\n",
    "* `filepath`: filepath is the complete address of the dataset or file that you\n",
    "are trying to read. The complete address includes the address of the directory\n",
    "in which the file is stored and the full name of the file with its extension.\n",
    "Remember to use a forward slash (/) in the directory address. Later in this\n",
    "chapter, we will see that the filepath can be a URL as well.\n",
    "* `sep:` sep allows us to specify the delimiter for the dataset to read. By default,\n",
    "the method assumes that the delimiter is a comma (,). The various other\n",
    "delimiters that are commonly used are blank spaces ( ), tab (|), and are called\n",
    "space delimiter or tab demilited datasets. This argument of the method also\n",
    "takes regular expressions as a value.\n",
    "* `dtype:` Sometimes certain columns of the dataset need to be formatted to\n",
    "some other type, in order to apply certain operations successfully. One\n",
    "example is the date variables. Very often, they have a string type which\n",
    "needs to be converted to date type before we can use them to apply daterelated\n",
    "operations. The dtype argument is to specify the data type of the\n",
    "columns of the dataset. Suppose, two columns a and b, of the dataset need to\n",
    "be formatted to the types int32 and float64; it can be achieved by passing\n",
    "`{'a':np.float64, 'b'.np.int32}` as the value of dtype. If not specified, it\n",
    "will leave the columns in the same format as originally found.\n",
    "* `header:` The value of a header argument can be an integer or a list.\n",
    "Most of the times, datasets have a header containing the column names.\n",
    "The header argument is used to specify which row to be used as the header.\n",
    "By default, the first row is the header and it can be represented as header\n",
    "=0. If one doesn't specify the header argument, it is as good as specifying\n",
    "header=0. If one specifies header=None, the method will read the data\n",
    "without the header containing the column names.\n",
    "* `names:` The column names of a dataset can be passed off as a list using this\n",
    "argument. This argument will take lists or arrays as its values. This\n",
    "argument is very helpful in cases where there are many columns and the\n",
    "column names are available as a list separately. We can pass the list of\n",
    "column names as a value of this argument and the column names in the list\n",
    "will be applied.\n",
    "* `skiprows:` The value of a skiprows argument can be an integer or a list.\n",
    "Using this argument, one can skip a certain number of rows specified as the\n",
    "value of this argument in the read data, for example skiprows=10 will read\n",
    "in the data from the 11th row and the rows before that will be ignored.\n",
    "* `index_col:` The value of an index_col argument can be an integer or a\n",
    "sequence. By default, no row labels will be applied. This argument allows\n",
    "one to use a column, as the row labels for the rows in a dataset.\n",
    "* `skip_blank_lines:` The value of a skip_blank_lines argument takes\n",
    "Boolean values only. If its value is specified as True, the blank lines are\n",
    "skipped rather than interpreting them as NaN (not allowed/missing values;\n",
    "we shall discuss them in detail soon) values. By default, its value is set\n",
    "to False.\n",
    "* `na_filter:` The value of a na-filter argument takes Boolean values only.\n",
    "It detects the markers for missing values (empty strings and NA values)\n",
    "and removes them if set to False.\n",
    "#### Use cases of the read_csv method\n",
    "\n",
    "#### Passing the directory address and filename as variables\n",
    "\n",
    "#### Reading a .txt dataset with a comma delimiter\n",
    "\n",
    "#### Specifying the column names of a dataset from a list\n",
    "\n",
    "### Case 2 – reading a dataset using the open method of Python\n",
    "\n",
    "#### Reading a dataset line by line\n",
    "\n",
    "#### Changing the delimiter of a dataset\n",
    "\n",
    "### Case 3 – reading data from a URL \n",
    "\n",
    "### Case 4 – miscellaneous cases\n",
    "\n",
    "#### Reading from an .xls or .xlsx file\n",
    "\n",
    "#### Writing to a CSV or Excel file\n",
    "\n",
    "### Basics – summary, dimensions, and structure\n",
    "\n",
    "### Handling missing values\n",
    "\n",
    "### Checking for missing values\n",
    "```\n",
    "pd.isnull(data['body'])\n",
    "pd.notnull(data['body'])\n",
    "pd.isnull(data['body']).values.ravel().sum()\n",
    "pd.nottnull(data['body']).values.ravel().sum()\n",
    "```\n",
    "\n",
    "### How missing values are generated and propagated\n",
    "\n",
    "### Treating missing values\n",
    "\n",
    "#### Deletion\n",
    "```\n",
    "data.dropna(axis=0,how='all')\n",
    "data.dropna(axis=0,how='any')\n",
    "\n",
    "```\n",
    "#### Imputation\n",
    "```\n",
    "data.fillna(0)\n",
    "data.fillna('missing')\n",
    "data['body'].fillna(0)\n",
    "```\n",
    "\n",
    "### Creating dummy variables\n",
    "```\n",
    "dummy_sex=pd.get_dummies(data['sex'],prefix='sex')\n",
    "\n",
    "column_name=data.columns.values.tolist()\n",
    "column_name.remove('sex')\n",
    "data[column_name].join(dummy_sex)\n",
    "```\n",
    "\n",
    "### Visualizing a dataset by basic plotting\n",
    "```\n",
    "import pandas as pd\n",
    "data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Customer Churn Model.txt')\n",
    "\n",
    "figure.savefig('E:/Personal/Learning/Predictive Modeling Book/Book\n",
    "Datasets/Scatter Plots.jpeg')\n",
    "```\n",
    "\n",
    "#### Scatter plots\n",
    "```\n",
    "data.plot(kind='scatter',x='Day Mins',y='Day Charge')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "figure,axs = plt.subplots(2, 2,sharey=True,sharex=True)\n",
    "data.plot(kind='scatter',x='Day Mins',y='Day Charge',ax=axs[0][0])\n",
    "data.plot(kind='scatter',x='Night Mins',y='Night Charge',ax=axs[0][1])\n",
    "data.plot(kind='scatter',x='Day Calls',y='Day Charge',ax=axs[1][0])\n",
    "data.plot(kind='scatter',x='Night Calls',y='Night Charge',ax=axs[1][1])\n",
    "```\n",
    "#### Histograms\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(data['Day Calls'],bins=8)\n",
    "plt.xlabel('Day Calls Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Day Calls')\n",
    "```\n",
    "\n",
    "#### Boxplots\n",
    "If the numbers in a distribution with 100 numbers are arranged in an\n",
    "increasing order; the 1st quartile will occupy the 25th position, the 3rd\n",
    "quartile will occupy the 75th position, and so on. The median will be the\n",
    "average of the 50th and 51st terms. (I hope you brush up on some of the\n",
    "statistics you have read till now because we are going to use a lot of it, but\n",
    "here is a small refresher). Median is the middle term when the numbers\n",
    "in the distribution are arranged in the increasing order. Mode is the one\n",
    "that occurs with the maximum frequency, while mean is the sum of all the\n",
    "numbers divided by their total count.\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(data['Day Calls'])\n",
    "plt.ylabel('Day Calls')\n",
    "plt.title('Box Plot of Day Calls')\n",
    "```\n",
    "\n",
    "## Summary\n",
    "The main learning outcomes of this chapter are summarized as follows:\n",
    "\n",
    "* Various methods and variations in importing a dataset using pandas: read_csv and its variations, reading a dataset using open method in Python, reading a file in chunks using the open method, reading directly from a URL, specifying the column names from a list, changing the delimiter of a dataset, and so on.\n",
    "\n",
    "* Basic exploratory analysis of data: observing a thumbnail of data, shape,column names, column types, and summary statistics for numerical variables\n",
    "\n",
    "* Handling missing values: The reason for incorporation of missing values, why it is important to treat them properly, how to treat them properly by deletion and imputation, and various methods of imputing data.\n",
    "\n",
    "* Creating dummy variables: creating dummy variables for categorical variables to be used in the predictive models.\n",
    "\n",
    "* Basic plotting: scatter plotting, histograms and boxplots; their meaning and relevance; and how they are plotted.\n",
    "\n",
    "This chapter is a head start into our journey to explore our data and wrangle it to make it modelling-worthy. The next chapter will go deeper in this pursuit whereby we will learn to aggregate values for categorical variables, sub-set the dataset, merge\n",
    "two datasets, generate random numbers, and sample a dataset.\n",
    "\n",
    "Cleaning, as we have seen in the last chapter takes about 80% of the modelling time, so it's of critical importance and the methods we are learning will come in handy in the pursuit of that goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./Chapter2/titanic3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './Chapter2/'\n",
    "filename = 'titanic3.csv'\n",
    "fullpath = path+'/'+filename\n",
    "data = pd.read_csv(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "path = 'C:/kappa/predictive-analytics-with-python/Chapter2'\n",
    "filename = 'titanic3.csv'\n",
    "fullpath = os.path.join(path,filename)\n",
    "data = pd.read_csv(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:/kappa/predictive-analytics-with-python/Chapter2/Customer Churn Model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifying the column names of a dataset from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:/kappa/predictive-analytics-with-python/Chapter2/Customer Churn Model.txt')\n",
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 2 – reading a dataset using the open method of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=open('C:/kappa/predictive-analytics-with-python/Chapter2/Customer Churn Model.txt','r')\n",
    "cols=data.next().strip().split(',')\n",
    "no_cols=len(data.next().strip().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Case 3 – reading data from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "medal_data=pd.read_csv('http://winterolympicsmedals.com/medals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "url='http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    for rows in response:\n",
    "        print (rows)    \n",
    "            \n",
    "cr=csv.reader(response)\n",
    "for rows in cr:\n",
    "    print (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a CSV or Excel file\n",
    "A data frame can be written in a CSV or an Excel file using a to_csv or to_excel\n",
    "method in pandas. Let's go back to the df data frame that we created in Case 2 –\n",
    "reading a dataset using the open method of Python. This data frame can be exported to a\n",
    "directory in a CSV file, as shown in the following code:\n",
    "```\n",
    "df.to_csv('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Customer Churn Model.csv'\n",
    "```\n",
    "Or to an Excel file, as follows:\n",
    "```\n",
    "df.to_excel('E:/Personal/Learning/Predictive Modeling Book/Book Datasets/Customer Churn Model.csv'\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
